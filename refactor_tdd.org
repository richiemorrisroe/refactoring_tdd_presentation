#+SETUPFILE: ~/Dropbox/org-minimal-html-theme/org-minimal-theme.setup
#+LaTeX_CLASS: beamer
#+LaTeX_CLASS_OPTIONS: [presentation]
#+BEAMER_THEME: Berkeley
#+COLUMNS: %45ITEM %10BEAMER_env(Env) %10BEAMER_act(Act) %4BEAMER_col(Col) %8BEAMER_opt(Opt)
#+PROPERTY: BEAMER_col_ALL 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0.0 :ETC
#+BIND: org-latex-image-default-width 0.7\linewidth
#+BIND: org-latex-image-default-height 0.7\linewidth
#+PROPERTY: header-args:R :session *R*  :eval no-export :height 360 :width 360
#+OPTIONS: tasks:nil
#+OPTIONS: toc:nil
#+OPTIONS: ^:nil
* Blurb :noexport:

Title: Seeing the Light with Refactoring and Testing: A guide for the data professional
- Tired of re-running scripts? 
- Confused when you return to a project/analysis six months later?
- Mystified by the decisions that others (including past you) have made in your code-base?
- Tormented by mysterious failures? 

This talk is for the practical data person who doesn't have time for software engineering,
but whose output is code. 

We'll cover the motivation, and some definitions of legacy code;
followed by a pitch for the tools to solve these problems: refactoring
and testing; concluded with some advice on a better strategy for your
future projects (Test-Driven Development).

Speaker: 
Richie has been fighting with legacy code (often his own) for
many years, and has recently found a religion that helps him to make
sense of all the complexity of modern data and software. Join him on a
journey to happiness and enlightment.

* Refactorings in R

Shamelessly stolen from the [[https://martinfowler.com/articles/refactoring-2nd-ed.html][book]].

He suggests that these be translated into other languages, 
python has lots of examples but R has very few. 



* Data Scientists and Code
- An awful lot of us produce our results/analyses through code
- Much of this is ad-hoc investigations
- Often these investigations produce output which others want
- You create a cron job
- Congratulations, you are now supporting production code [fn:2]
* My background
- I trained as a psychologist
- I like numbers
- So I ended up as a data scientist
- At no point have I ever had a class on coding
- I haven't even had that many classes on statistics
* So why are you even doing this talk?
- Because I have learned, from bitter, bitter experience that this
  stuff is important
- People (hopefully) make decisions as a result of our analyses
- Often, these decisions can have far-ranging impacts, most of which
  are impossible [fn:3] to foresee
- As professionals, we have an obligation to make sure these results
  are correct

- This means we need to make sure our code is correct

* This Talk
- I will talk about tools to improve the quality of code, and our
  confidence in it
- These tools are:
  - Automated testing
  - Refactoring
  - Test-Driven Development

* Before Refactoring
- You *must* write tests
- Otherwise you [fn:1] *will* introduce bugs
* Types of Testing
- Individual functions: unit testing
- Overall application: functional testing
- Lots of space between these two points

* My Code is so awful that it can't be tested
- This is depressingly normal
- You want some kind of characteristic test
- This is often a dataframe type structure
- The quickest test is to ensure that both dataframes are equal
- There is a /wonderful/ book that guides you through this
* Working Effectively with Legacy Code
- [[https://www.amazon.co.uk/Working-Effectively-Legacy-Michael-Feathers/dp/0131177052][The book]], by Michael Feathers is really useful
- Legacy code = code without tests
- Book is based on particular problems
- And provides approaches for solving them
- Catolog of refactorings that can (theoretically) be done without
  tests [fn:4]
* Characterisation Tests
#+begin_src R :session :eval no
  require(testthat)
  output_old <- readr::read_csv(
                         "older_data.csv",
                         ## you'll need this argument
                         col_types=NULL) 
  test_that('output old = output new',
            all.equal(output_old, output_new))
#+end_src
- capture how the system behaves right now, and make sure it doesn't change
- This simple test can allow you to make a /lot/ of progress relatively quickly
- this tests are useful to help build structure and get other tests in place
- but they prevent you from improving the code in pretty much any way
* Seams
- This is one of the most interesting parts of WELC, which defines a
  seam as a place where you can alter what code runs without changing any code
- For instance, if you have some kind of folder with different
  versions of code then you can create a seam by changing which folder is pointed to
- This is also a very quick way of getting characterisation tests
- The simplest example is mocking out a package/script file/module by changing
  the path
* Pinch Points
- Another concept from WELC
- Refers to the point where you can test as much as possible of functionalities
- Normally a dataframe of some description (input data, output data,
  predictions etc) 
* First Steps
- Gather the source files you are interested in
- Run them, make sure they work. 
- DO NOT SKIP THIS STEP!!!
* My Example
- Its based on the irish property market data
- I have been working on and off on this for a number of years
- I have multiple folders, datasets and scripts that perform data
  loading, processing and modelling
- I now want to turn this into an interactive app, and keep it updated
- I aim to add tests and refactor to make it easier for me to build on
  top of this foundation
* First Practical Step
   #+begin_src R :session :results none :tangle r-build.R
     require(devtools)
     if (!dir.exists("ppr")) {
       usethis::create_package("ppr")
     }

     usethis::use_package("dplyr")
     usethis::use_package("rlang")
     usethis::use_package("readxl")
     usethis::use_package("caret")
     usethis::use_package("rgdal")
     usethis::use_package("sp")
     usethis::use_package("glmnet")
     usethis::use_package("sf")
     usethis::use_data_raw()
   #+end_src

- This creates an R package for us

* Why a package?
- Lots of good things:
  - automated dependency management
  - easy documenting facilities (with roxygen2)
  - easy to run tests (with testthat)
  - check is just a bunch of better programming practices

- Bad things:
  - more effort
  - appeasing the mighty gods of check takes time
* Next steps
- We have three scripts:
  - prep_modelling.R
  - feature_engineering.R
  - models.R
* Step 0: Create a custom environment
- I use [[https://docs.conda.io/en/latest/][conda]] for this
- It supports multiple languages, and compiled dependencies
- This is pretty awesome for python, and its a little more
  light-weight than Docker
- It supports R (and apparently loads of other languages too)
#+begin_src sh
conda create -n PPR-r r=3.6.3 
conda env export --name PPR-r > PPR-r.yml
#+end_src
* First, generate or create ground truth data
- Our scripts need to be run in order
- So, we can use the boundaries between them as points where we can
  introduce test points
- However, our scripts suck and therefore we don't have any convenient
  boundaries available

#+begin_src 
rm(list=ls())
source("scripts/prep_modelling.R")
#+end_src
- Simplest way to run script
- We can then look at what it produces and decide what to test
- Biggest benefit here is not having to change anything
* Assessing what we need
   #+begin_src R :session :exports code
   ls()
   #+end_src

- We have a bunch of dataframes, some functions and a few intermediate results
- However, the best *pinch point* is ppr_train and test, as they are
  downstream of all our feature engineering

* Add data to package
#+begin_src R :tangle ppr/data-raw/DATASET.R
ppr <- readxl::read_excel("~/Dropbox/PPR/PPR-ALL.xlsx", sheet = "PPR-ALL")
set.seed(49)
ppr_sample <- dplyr::sample_frac(ppr, size = 0.05)
ppr <- ppr_sample
usethis::use_data(ppr, overwrite = TRUE)
#+end_src
- This is important for running the tests
- You /can/ use absolute paths and stuff, but it's pretty ugly
- CRAN strongly suggests that data be less than 5Mb, so we take a 5%
  sample
* Create pinch point
- We need to run our script with the package data, so we can keep
  everything consistent
- We add all the scripts to inst/, which is where we'll put the test
  data also
#+begin_src R :session
readr::write_csv(ppr_train2, path = "prep_modelling_output_old.csv")
#+end_src
- We put this at the end of our script
- Then, we run it like so:
#+begin_src sh
cd ppr/inst/
Rscript prep_modelling.R
#+end_src
- We then output the full file
- However, because we took a 5% sample earlier, we need to change things
* Handling the sample
- We have the data in our package [fn:7]
- We need to change our script to use the package data
- We can either rename the script, or rely on version control 
#+begin_src sh
cp prep_modelling.R prep_modelling_refactor.R
#+end_src
- Then we rename our output file
#+begin_src R :session
  ## readr::write_csv(ppr_train2, path = "prep_modelling_output_old.csv")
  readr::write_csv(ppr_train2, path = "prep_modelling_output_refactor.csv")
#+end_src
* Commented out code is bad, right?
- In general commented out code is a massive anti-pattern
- But right now, we are making minimal changes to ensure that we can
  safely update our script
* Import data from package
  #+begin_src R :session
    ## library(tidyverse)
    ## library(readxl)
    ## ppr <- read_excel("~/Dropbox/PPR/PPR-ALL.xlsx", sheet = "PPR-ALL")
    require(ppr)
    data(ppr)
    names(ppr)
  #+end_src

#+begin_src sh :results none
  mkdir ppr/tests
  mkdir ppr/tests/testthat/
  touch ppr/tests/testthat/test_first.R
  touch ppr/tests/testthat/test_integration_prep_modelling.R
#+end_src

* Integration Test
  #+begin_src R :session :tangle ppr/tests/testthat/test_integration_prep_modelling.R
  context("integration test prep_modelling")
old <- readr::read_csv("../../inst/prep_modelling_output_old.csv")
new <- readr::read_csv("../../inst/prep_modelling_output_refactor.csv")
test_that(
        "data.frame outputs are equal",
        expect_equal(old, new)
)
  #+end_src

* Run the script again with the changes we've made
- Commit your code before and after!
- We actually needed to make the data changes /first/ as otherwise our
  "old" data wouldn't match the new(er) data
* Create a small script to run the integration tests
- This talk is all about *automated* testing, so we'll write a script
#+begin_src R :session :tangle ppr/inst/integration_test_prep_modelling.R
  system("Rscript prep_modelling.R")
  testthat::test_file("../tests/testthat/test_integration_prep_modelling.R")
#+end_src

* First Steps First
- Move the functions we have to a file
- Change our script to load the functions from this file
#+begin_src R :session :tangle ppr/R/ppr.R :exports none
  ##' remove non ASCII characters from name vector of df
  ##'
  ##' replaces spaces with underscores
  ##' @title normalise_names
  ##' @param df an object inheriting from data.frame
  ##' @return a data.frame with fixed names
  ##' @author richie
  ##' @export
  normalise_names <- function(df) {
          nms <- names(df)
          normed <- iconv( # covert from one encoding to another
                  tolower(
                          gsub("([[:space:]]|[[:punct:]])+", "_",
                                  x = nms
                          )
                  ),
                  "latin1", # from encoding
                  "ASCII", # to encoding
                  sub = ""
          )
          drop_usc <- gsub("([a-z_])_*$",
                           "\\1", x = normed) # drop extra underscores
          names(df) <- drop_usc # update names
          df # return dataframe
  }
  fix_price <- function(x) {
          nopunct <- gsub(",", "", x = x)
          nums <- as.numeric(
                  iconv(nopunct, "latin1",
                          "ASCII",
                          sub = ""
                  )
          )
  }
  ##' fit a simple model, bootstrap the results
  ##'
  ##' see above
  ##' @title generate_bootstrap_results
  ##' @param df a data.frame
  ##' @param ind a set of indices representing which observations to retain
  ##' @return a list of results
  ##' @author richie
  ##' @export
  generate_bootstrap_results <- function(df, ind) {
          # results list (always generate something to hold your results first)
          trainresults <- vector(mode = "list", length = 10)
          for (i in 1:length(ind)) {
                  nm <- names(ind[i])
                  train <- df[ind[[i]], ]
                  test <- df[-ind[[i]], ]
                  model <- lm(log_price ~ property_size_description + year,
                          data = train
                  )
                  preds <- predict(model,
                          newdata = test, type = "response",
                          na.action = na.exclude
                  )
                  trainresults[[i]] <- preds
          }
          names(trainresults) <- names(ind)
          trainresults
  }

#+end_src
- We now have a package with code in it
- Now we run our integration test again
- Next step is to add some unit tests
* Write some tests

#+begin_src R :tangle ppr/tests/testthat/test_first.R
  require(testthat)
  context("load data")
  dat  <- load_data("~/Dropbox/PPR/PPR-ALL.xlsx")
  test_that('load_data returns a tibble',
    {expect_equal(
              class(dat)[1], "tbl_df")})
#+end_src
- The first thing we do is write a load_data function
- this allows us to later abstract how/where we load the data from
  without needing to change (as much code)
- Note that this function doesn't exist yet
- Let's write it
* Loading Data

#+begin_src R :session :tangle ppr/R/ppr.R :exports none
  load_data <- function(path) {
    ppr <- readxl::read_excel(path, sheet = "PPR-ALL")
    return(ppr)
    }
#+end_src
- This wouldn't normally make a whole lot of sense
- But I know that I am probably going to move to SQL storage with this
  project
- So centralising the data loading does make sense [fn:6]
* Test Names
   #+begin_src R :session :tangle ppr/tests/testthat/test_first.R
     finished_names <-
       c("date_of_sale_dd_mm_yyyy",
         "address", "postal_code", "county",                   
         "price", "not_full_market_price","vat_exclusive",
         "description_of_property",  
         "property_size_description") %>%
       as.character()

     test_that('normalise_names works',
     {expect_equal(finished_names,
                   names(normalise_names(dat)))
     })
   #+end_src
- We just grab the output of normalise_names, and test that
- this is a shitty test; try to test the actual invariants on edge cases
* Test Fix Price
   #+begin_src R :tangle ppr/tests/testthat/test_first.R

     test_that('fix price returns the correct price',
               expect_equal(3e6, fix_price("3,000,000")))
   #+end_src
- All good
- However, I think I spotted a bug in fix_price
* Fix Price Bug
   #+begin_src R :exports code
     fix_price <- function(x) {
             nopunct <- gsub(",|\\.", "", x = x)
             nums <- as.numeric(
                     iconv(nopunct, "latin1",
                             "ASCII",
                             sub = ""
                     )
             )
     }
   #+end_src
- Can you spot it?
* Test Case for Bug
   #+begin_src R :session :tangle ppr/tests/testthat/test_first.R

     test_that('fix price returns the correct price',
               expect_equal(3000000.01,
                            fix_price("3,000,000.01")))

   #+end_src
- We actually shouldn't strip the dots, as they are valid in numbers
- We have a failing test, so we can fix this code (and make sure it
  stays fixed, which is normally more valuable)
* Fix Price Fix
   #+begin_src R :exports code :tangle ppr/R/ppr.R
     ##' fix_price
     ##'
     ##' remove commas from a vector. Normally used to handle numbers with commas
     ##' @title fix_price
     ##' @param x a vector 
     ##' @return a numeric vector
     ##' @author richie
     ##' @export
     fix_price <- function(x) {
             nopunct <- gsub(",", "", x = x)
             nums <- as.numeric(
                     iconv(nopunct, "latin1",
                             "ASCII",
                             sub = ""
                     )
             )
     }
   #+end_src
- We just remove the part of the gsub call that matches dots
- Now all our tests pass
* Generate Bootstrap Results
- I'll leave testing this for now, as I probably need to do the
  feature normalisation first
* Break out components
- You want to break out chunks of code into manageable (well-named) sizes
- This is the extract function refactoring (probably the most common)
#+begin_src R :exports code
  ppr3 <- mutate(ppr2,
                 price = price / 100,
                 is_big = ifelse(price >= 2e6, "Big", "Not Big"))
#+end_src

* Write test for new function
  #+begin_src R :tangle ppr/tests/testthat/test_first.R
    test_df_output  <- readr::read_csv("~/Dropbox/Code/Rlang/refactoring_and_tdd/ppr/mark_values_as_large_test_data_done.csv")
    test_df_input  <- readr::read_csv("~/Dropbox/Code/Rlang/refactoring_and_tdd/ppr/mark_values_as_large_test_data.csv")
    test_that('mark values as large works', {
      expect_equal(mark_values_as_large(test_df_input, large = 1e6),
                   test_df_output)
    })

  #+end_src
* Write the actual function
#+begin_src R :tangle ppr/R/ppr.R
  mark_values_as_large <- function(df, large) {
    large <- rlang::enquo(large)
    ppr3 <- dplyr::mutate(df,
                          is_big = ifelse(.data$price >= !!large,
                                          "Big", "Not Big"))
    return(ppr3)
    }
#+end_src
- By which I mean copying code into a function and adding a return statement
- This is the *extract function* refactoring
* Opposite but Equal
- Sometimes, we actually want the function to be inlined
- This is called *inline function*
* Convert price to log
- Logs are great
- In general, if you have problems with a response or predictor
  variable, you should log it and see if that helps
#+begin_src R :session
ppr4 <- mutate(ppr3, log_price = log(price, base = 10))
#+end_src
- Does this need to be a function?
* Log function tests 
   #+begin_src R :tangle ppr/tests/testthat/test_first.R
     test_that('log(price)< price',
     {
       data(ppr)
       ppr2 <- normalise_names(ppr) %>% dplyr::mutate(price=fix_price(price))
       new_df <-  log_column(ppr2, price)
       expect_lt(new_df$log_price[1],
                         ppr2$price[1])}
               )
   #+end_src
- Obviously this fails again
- One of the great things about TDD is that you end up needing to use
  your API somewhere before you write it
- This provides both documentation and a usability check
* Log Column Function
- Again, we need to do the pointless tidyverse dance because Hadley hates quotes
#+begin_src R :tangle ppr/R/ppr.R
  log_column <- function(df, col) {
      col <- rlang::enquo(col)
      res <- dplyr::mutate(df, log_price = log(!!col, base = 10))
      return(res)
      }
#+end_src
- This is beginning to annoy me a little less, from repetition
* What not to functionalise
- Exploratory analysis
- There's loads of that in my scripts, for example
#+begin_src R :session
post_codes_table <-
        with(ppr4, table(postal_code, useNA = "always")) %>%
        as.data.frame() %>%
        arrange(desc(Freq))
head(post_codes_table, n = 12)
#+end_src
- This is to figure out what's going on
- Not sure it makes a lot of sense here
- If I'm doing the same stuff on a bunch of columns, a function is
  useful, but otherwise it doesn't make much sense
* Next Steps
   #+begin_src R :session
     ppr5 <- mutate(ppr4,
             is_full_market_price = ifelse(
                     not_full_market_price == "No",
                     "Yes",
                     "No"
             )
     ) %>%
             select(-not_full_market_price) # remove old var - what happens if we don't?
   #+end_src
- So this is only done once, and is pretty specific
- But it handles some ugly, ugly names
- This is a *rename field* refactoring (which can be incredibly impactful)
* Rename Field Test
- First, we write a test
#+begin_src R :tangle ppr/tests/testthat/test_first.R
  data(ppr)
  test_that('we have is_full_market_price column', {
            ppr3 <- normalise_names(ppr) %>%
              dplyr::mutate(price=fix_price(price)) %>%
              log_column(price)
            ppr4 <- invert_field(ppr3, not_full_market_price)
            expect_equal(names(ppr4)[length(ppr4)], "is_full_market_price") }
            )

#+end_src
- Which obviously fails
- I did reconsider my name after the failure (the original name was
  fix_field_names, but that was needlessly vague)
* Invert Field Names
   #+begin_src R :tangle ppr/R/ppr.R
     invert_field <- function(df, field) {
       field  <- rlang::enquo(field)
       ppr5 <- dplyr::mutate(df,
                  is_full_market_price = ifelse(
                          !!field == "No",
                          "Yes",
                          "No"
                  )
                  ) %>%
         dplyr::select(-not_full_market_price)
       return(ppr5)
     }
   #+end_src
- This is a pretty crap function, but it's a better grounding for future development
- Remember that once we have tests on everything, we can refactor much more fearlessly
* Description of property field
- When I wrote this code (for a data science bootcamp) I discovered
  that this field is buggy and annoying
- There's also a bunch of entries as Gaeilge, which is annoying for my purposes
- I have about 30 lines of logic here, most of which won't be reusable right away. 
- The easiest thing to do with it is just dump it all into a function
  so we can test it
* Property Description Function
  #+begin_src R :session :tangle ppr/R/ppr.R :exports none
    fix_property_description  <- function(df) {
    remove_irish <- dplyr::mutate(df, prop_description = ifelse(
            grepl("cothrom", x = property_size_description),
            "greater than or equal to 38 sq metres and less than 125 sq metres",
            ifelse(
                    grepl("cearnach", x = property_size_description),
                    "less than 38 sq metres",
                    property_size_description
            )
            ))

    shorten_greater_than <- dplyr::mutate(remove_irish, prop_description = ifelse(
            prop_description == "greater than 125 sq metres",
            "ge_125_square_meters",
            ifelse(
                    prop_description ==
                            "greater than or equal to 125 sq metres",
                    "ge_125_square_meters",
                    prop_description
            )
    ))

    shorten_less_than_greater_than <-
      dplyr::mutate(shorten_greater_than,
                    property_size_description = ifelse(
                      prop_description == "less than 38 sq metres",
                      "lt_38_square_meters",
                                                ifelse(
                                                  prop_description ==
                                                  "greater than or equal to 38 sq metres and less than 125 sq metres",
                                                  "ge_38_lt_125_square_meters",
                                                  prop_description
                                                )
                    )) %>%
            dplyr::select(-prop_description) # pointless now
    result <- dplyr::mutate(shorten_less_than_greater_than,
                            property_size_description = as.character(
                              forcats::fct_explicit_na(property_size_description))
            )
    return(result)
    }
  #+end_src
* Testing our Property Description Logic
- Kinda painful
- Let's punt on it and just ensure that the same data is output
- This is something that's a really, really common occurence [fn:5]
#+begin_src R :session :tangle ppr/tests/testthat/test_first.R
  test_that('new property desc logic is the same as old',{
            data(ppr)
            ppr_input  <- normalise_names(ppr) %>%
              dplyr::mutate(price=fix_price(price)) %>%
              mark_values_as_large(1e6) %>%
              log_column(price) %>% 
              invert_field(not_full_market_price)
            ppr_old  <-
              readr::read_csv("~/Dropbox/Code/Rlang/refactoring_and_tdd/ppr/ppr_data_cleaning_done.csv") %>%
              dplyr::mutate(property_size_description=as.character(property_size_description))
            expect_equal(fix_property_description(ppr_input),
                         ppr_old)})
#+end_src

* Data Splitting
- At this point, we have created functions for each of our data
  cleaning steps
- The next section is handling test and train sets
- If I was doing this today, the recipe package would probably be my
  first port of call
#+begin_src R
library(caret)
set.seed(34)
ppr_train_indices <- with(
        ppr10,
        createDataPartition(log_price,
                times = 1,
                p = 0.7,
                list = FALSE
        )
)
ppr_train <- ppr10[ppr_train_indices, ]

ppr_not_train <- ppr10[-ppr_train_indices, ]
ppr_test_indices <- with(
        ppr_not_train,
        createDataPartition(log_price,
                times = 1,
                p = 0.5,
                list = FALSE
        )
) 

ppr_test <- ppr_not_train[ppr_test_indices, ]
ppr_validation <- ppr_not_train[ppr_test_indices, ]
write_csv(x = ppr_validation, path = "ppr_validation_set.csv")
rm(ppr_validation)
#+end_src

* Data Splitting Test
  #+begin_src R :session :tangle ppr/tests/testthat/test_first.R
    data(ppr)
    ppr_for_split  <- normalise_names(ppr) %>%
      dplyr::mutate(price=fix_price(price)) %>%
      mark_values_as_large(1e6) %>%
      log_column(price) %>% 
      invert_field(not_full_market_price) %>%
      fix_property_description()
    test_that('split_data returns a list',
              expect_is(split_data(ppr_for_split), 'list'))
  #+end_src
- Start with a simple test, make them more specific as times goes on
* Data Splitting Function

#+begin_src R 
  split_data <- function(df) {
    return(list())
    }
#+end_src
- Passes the test :)
* Splitting Data
- Dump all the code into a function
- Passes the tests
  #+begin_src R :session :tangle ppr/R/ppr.R
    split_data <- function(df) {
      set.seed(34)
      ppr_train_indices <- with(
        df,
        caret::createDataPartition(log_price,
                            times = 1,
                            p = 0.7,
                            list = FALSE
                            )
      ) %>% as.vector() #because tibble sucks
      ppr_train <- df[ppr_train_indices, ]

      ppr_not_train <- df[-ppr_train_indices, ]
      ppr_test_indices <- with(
        ppr_not_train,
        caret::createDataPartition(log_price,
                            times = 1,
                            p = 0.5,
                            list = FALSE
                            )
      ) %>% as.vector()

      ppr_test <- ppr_not_train[ppr_test_indices, ]
      ppr_validation <- ppr_not_train[-ppr_test_indices, ]
      readr::write_csv(x = ppr_validation, path = "ppr_validation_set.csv")
      rm(ppr_validation)
      return(list(train=ppr_train, test=ppr_test))
    }
  #+end_src
* Add More Tests
  #+begin_src R :session :tangle ppr/tests/testthat/test_first.R
    test_that('split data has test and train',
              expect_equal(names(split_data(ppr_for_split)), c("train", "test")))

    test_that('split data returns a train tibble',
              expect_is(split_data(ppr_for_split)$train[1], 'tbl_df'))

    test_that('split data returns a test tibble',
              expect_is(split_data(ppr_for_split)$test[1], 'tbl_df'))

    test_that('split_data train has less rows than input',{
              train  <- split_data(ppr_for_split)$train
              expect_gt(nrow(ppr_for_split), nrow(train))}
              )
  #+end_src
* Refactor :noexport:
- We now have more tests, so we can actually refactor
- The first (and most important) refactoring is to get a handle on
  global variables
- Because of R's lexical scope, these are often a source of latent
  (and horrible) bugs
** TODO come back to this later
* 

** Top Six Refactorings

*** Extract Function

*** Inline Function

*** Extract Variable

*** Inline Variable

*** Change Function Declaration

*** Encapsulate Variable

*** Rename Variable

*** Introduce Parameter Object

*** Combine Functions into Class

*** Combine Functions into Transform

*** Split Phase

* Footnotes

[fn:7] right now, it's the /only/ thing in the package  

[fn:6] given my use-cases 

[fn:5] especially with code you didn't write yourself 

[fn:4] you *always* want at least some characterisation tests (i.e. output) 

[fn:3] i.e. hard 

[fn:2] in the sense that others rely on it. SQL tables are often the
worst offenders (as they are often the easiest to create)

[fn:1] or at least me 
